#import os, sys
#sys.path.insert(0, os.path.abspath("../.."))
import tensorflow as tf
from models.base_model import BaseModel
from tflearn.layers import conv_2d, max_pool_2d,fully_connected, batch_normalization, \
        dropout

from tflearn.utils import get_incoming_shape

class MALWARE_CNN_V1(BaseModel):

    def _build_network(self, batch, type_ = 'train'):
        params = self.params

        img_batch, label_batch = batch

        if type_ == 'train':
            tf.summary.image(name="Malware", tensor=img_batch, 
                max_outputs=6, collections=[tf.GraphKeys.MYGRAPHS])

        input_shape = get_incoming_shape(img_batch)
        label_shape = get_incoming_shape(label_batch)

        img_batch = tf.cast(img_batch, tf.float32)
        img_batch = (img_batch - params.mean)/params.std

        with tf.variable_scope("conv1") as scope:
            net = conv_2d(img_batch, nb_filter = 32, filter_size = 5, strides = 1,
                    padding = 'same', activation = 'relu', weights_init='xavier',
                    regularizer = "L2", weight_decay = params.weight_decay)

        if type_ == 'train':
            tf.summary.histogram(name='conv1/relu', values=net, collections=[tf.GraphKeys.MYACTIVATIONS])
        #with tf.variable_scope("batch_norm1") as scope:
        #    net = batch_normalization(net)

        with tf.variable_scope("pool1") as scope:
            net = max_pool_2d(net, kernel_size = 2, strides=None, padding='same',
                    name="MaxPool2D")
        
        #if type_ == 'train':
        #    net = dropout(net, 0.8)


        with tf.variable_scope("conv2") as scope:
            net = conv_2d(net, nb_filter = 32, filter_size = 5, strides = 1,
                    padding = 'same', activation = 'relu', weights_init='xavier',
                    regularizer = "L2", weight_decay = params.weight_decay)  

        if type_ == 'train':
            tf.summary.histogram(name='conv2/relu', values=net, collections=[tf.GraphKeys.MYACTIVATIONS])
            #tf.add_to_collection(tf.GraphKeys.MYACTIVATIONS, net)
        with tf.variable_scope("pool2") as scope:
            net = max_pool_2d(net, kernel_size = 2, strides=None, padding='same',
                    name="MaxPool2D")

        with tf.variable_scope("conv3") as scope:
            net = conv_2d(net, nb_filter = 32, filter_size = 5, strides = 1,
                    padding = 'same', activation = 'relu', weights_init='xavier',
                    regularizer = "L2", weight_decay = params.weight_decay)

        #with tf.variable_scope("pool3") as scope:
        #    net = max_pool_2d(net, kernel_size = 2, strides=None, padding='same',
        #            name="MaxPool2D")

        with tf.variable_scope("fc1") as scope:
            net = fully_connected(net, n_units = 50, activation='relu',
                    regularizer = "L2", weight_decay = params.weight_decay)

        #if type_ == 'train':
        #    tf.summary.histogram(name='fc1/relu', values=net, collections=[tf.GraphKeys.MYACTIVATIONS])
            #tf.add_to_collection(tf.GraphKeys.MYACTIVATIONS, net)

        with tf.variable_scope("fc2") as scope:
            net = fully_connected(net, n_units = label_shape[1], activation='relu',
                    regularizer = "L2", weight_decay = params.weight_decay)

        if type_ == 'train':
            tf.summary.histogram(name='fc2/relu', values=net, collections=[tf.GraphKeys.MYACTIVATIONS])
            #tf.add_to_collection(tf.GraphKeys.MYACTIVATIONS, net)

        if type_ == 'test':
            self.prediction = tf.nn.softmax(net)
       
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, label_batch))
        correct_pred = tf.equal(tf.argmax(net, 1), tf.argmax(label_batch, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
        
        if type_ == 'train':

            self.loss = loss
            self.acc = accuracy
            tot_loss = loss + tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
            self.opt_op = tf.train.AdamOptimizer(learning_rate=params.learning_rate).minimize(tot_loss,
                    global_step = self.global_step)
            tf.summary.scalar(name='TRAIN_Loss', tensor=self.loss, collections=[tf.GraphKeys.MYSCALARS])
            tf.summary.scalar(name='TRAIN_Accuracy', tensor=self.acc, collections=[tf.GraphKeys.MYSCALARS])
        elif type_ == 'valid':
            if True:#params.task == 'local_validation':
                self.valid_loss = loss
                self.valid_acc = accuracy
                tf.summary.scalar(name='VALIDATION_Loss', tensor=self.valid_loss, collections=[tf.GraphKeys.MYSCALARS])
                tf.summary.scalar(name='VALIDATION_Accuracy', tensor=self.valid_acc, collections=[tf.GraphKeys.MYSCALARS])

        #print("Output shape:", get_incoming_shape(net), '\n')
